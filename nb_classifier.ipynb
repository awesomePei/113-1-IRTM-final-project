{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## likelihood\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # type: ignore\n",
    "from nltk.corpus import stopwords # type: ignore\n",
    "import nltk # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess(text):\n",
    "    \"\"\"Clean, tokenize, and remove stopwords from text.\"\"\"\n",
    "    import re\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    words = text.split()  # Tokenize text\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    # 移除第三行\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 2:\n",
    "        text = '\\n'.join(lines[:2] + lines[3:])\n",
    "    # 移除指定單詞和停用詞\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word != 'embed']\n",
    "    return ' '.join(tokens)\n",
    "    return filtered_words\n",
    "\n",
    "# Load Documents\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"Load documents from folder.\"\"\"\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        doc_id = int(filename.split('.')[0])  # Assume filenames are numbers\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            docs[doc_id] = preprocess(file.read())\n",
    "    return docs\n",
    "\n",
    "# Calculate TF-IDF and Reduce Vocabulary\n",
    "def calculate_tfidf(docs, max_features=1000):\n",
    "    \"\"\"Calculate TF-IDF and select top features.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(docs[doc_id]) for doc_id in sorted(docs.keys())])\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    return vocab\n",
    "\n",
    "# Calculate Likelihood Ratio for Feature Selection\n",
    "def calculate_likelihood_ratio(docs, labels, vocab):\n",
    "    \"\"\"Calculate Likelihood Ratio for each term-class pair.\"\"\"\n",
    "    term_class_counts = Counter()\n",
    "    term_counts = Counter()\n",
    "    class_counts = Counter(labels)\n",
    "    total_docs = len(labels)\n",
    "\n",
    "    # Count term occurrences in documents for each class\n",
    "    for i, doc in enumerate(docs):\n",
    "        unique_terms = set(doc)  # Consider unique terms in each document\n",
    "        for term in unique_terms:\n",
    "            if term in vocab:\n",
    "                term_class_counts[(term, labels[i])] += 1\n",
    "                term_counts[term] += 1\n",
    "\n",
    "    # Calculate Likelihood Ratios\n",
    "    likelihood_scores = defaultdict(float)\n",
    "    for term in vocab:\n",
    "        for cls in class_counts.keys():\n",
    "            observed = term_class_counts.get((term, cls), 0)\n",
    "            expected = (term_counts[term] * class_counts[cls]) / total_docs\n",
    "            if expected > 0:  # Avoid division by zero\n",
    "                likelihood_scores[term] += ((observed - expected) ** 2) / expected\n",
    "\n",
    "    # Sort terms by their likelihood scores\n",
    "    return sorted(likelihood_scores, key=likelihood_scores.get, reverse=True)\n",
    "\n",
    "# Train Multinomial Naive Bayes\n",
    "def train_naive_bayes(class_docs, docs, selected_vocab):\n",
    "    \"\"\"Train Multinomial Naive Bayes with add-one smoothing.\"\"\"\n",
    "    class_counts = Counter()\n",
    "    term_counts = {cls: Counter() for cls in class_docs.keys()}\n",
    "    vocab_size = len(selected_vocab)\n",
    "\n",
    "    # Count terms for each class\n",
    "    for cls, doc_ids in class_docs.items():\n",
    "        class_counts[cls] += len(doc_ids)\n",
    "        for doc_id in doc_ids:\n",
    "            for term in docs[doc_id]:\n",
    "                if term in selected_vocab:\n",
    "                    term_counts[cls][term] += 1\n",
    "\n",
    "    # Calculate probabilities\n",
    "    class_probs = {cls: math.log(class_counts[cls] / sum(class_counts.values())) for cls in class_counts}\n",
    "    term_probs = {cls: defaultdict(float) for cls in class_counts}\n",
    "\n",
    "    for cls in class_counts:\n",
    "        total_terms = sum(term_counts[cls].values()) + vocab_size\n",
    "        for term in selected_vocab:\n",
    "            term_probs[cls][term] = math.log((term_counts[cls][term] + 1) / total_terms)\n",
    "\n",
    "    return class_probs, term_probs\n",
    "\n",
    "# Classify Documents\n",
    "def classify_document(doc, selected_vocab, class_probs, term_probs):\n",
    "    \"\"\"Classify a single document.\"\"\"\n",
    "    scores = {cls: class_probs[cls] for cls in class_probs}\n",
    "    for cls in class_probs:\n",
    "        for term in doc:\n",
    "            if term in selected_vocab:\n",
    "                scores[cls] += term_probs[cls][term]\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Classify Testing Documents with TF-IDF and Likelihood Ratio\n",
    "def classify_and_output_with_tfidf_likelihood(training_data, docs, output_path, tfidf_features=1000, lr_features=500):\n",
    "    \"\"\"Classify testing documents using TF-IDF and Likelihood Ratio-based feature selection.\"\"\"\n",
    "    # Parse training data\n",
    "    class_docs = defaultdict(list)\n",
    "    labels = []\n",
    "    train_doc_ids = []\n",
    "    for line in training_data.strip().split(\"\\n\"):\n",
    "        parts = line.split()\n",
    "        cls = int(parts[0])\n",
    "        docs_in_class = list(map(int, parts[1:]))\n",
    "        class_docs[cls].extend(docs_in_class)\n",
    "        labels.extend([cls] * len(docs_in_class))\n",
    "        train_doc_ids.extend(docs_in_class)\n",
    "\n",
    "    train_docs = [docs[doc_id] for doc_id in train_doc_ids]\n",
    "\n",
    "    # Identify testing documents (all IDs not in training set)\n",
    "    all_doc_ids = set(docs.keys())\n",
    "    test_doc_ids = sorted(all_doc_ids - set(train_doc_ids))\n",
    "    test_docs = {doc_id: docs[doc_id] for doc_id in test_doc_ids}\n",
    "\n",
    "    # Step 1: Calculate TF-IDF and Reduce Vocabulary\n",
    "    tfidf_vocab = calculate_tfidf({doc_id: docs[doc_id] for doc_id in train_doc_ids}, max_features=tfidf_features)\n",
    "\n",
    "    # Step 2: Perform Likelihood Ratio on TF-IDF-reduced vocabulary\n",
    "    lr_vocab = calculate_likelihood_ratio(train_docs, labels, tfidf_vocab)[:lr_features]\n",
    "\n",
    "    # Train Naive Bayes on LR-selected features\n",
    "    class_probs, term_probs = train_naive_bayes(class_docs, docs, lr_vocab)\n",
    "\n",
    "    # Classify testing documents\n",
    "    test_predictions = {}\n",
    "    for doc_id, doc in test_docs.items():\n",
    "        predicted_class = classify_document(doc, lr_vocab, class_probs, term_probs)\n",
    "        test_predictions[doc_id] = predicted_class\n",
    "\n",
    "    # Write results to CSV\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Id,Value\\n\")  # Write header\n",
    "        for doc_id in sorted(test_predictions.keys()):\n",
    "            file.write(f\"{doc_id},{test_predictions[doc_id]}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = \"\"\"\n",
    "    1 11 19 29 113 115 169 278 301 316 317 321 324 325 338 341\n",
    "    2 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16\n",
    "    3 813 817 818 819 820 821 822 824 825 826 828 829 830 832 833\n",
    "    4 635 680 683 702 704 705 706 708 709 719 720 722 723 724 726\n",
    "    5 646 751 781 794 798 799 801 812 815 823 831 839 840 841 842\n",
    "    6 995 998 999 1003 1005 1006 1007 1009 1011 1012 1013 1014 1015 1016 1019\n",
    "    7 700 730 731 732 733 735 740 744 752 754 755 756 757 759 760\n",
    "    8 262 296 304 308 337 397 401 443 445 450 466 480 513 533 534\n",
    "    9 130 131 132 133 134 135 136 137 138 139 140 141 142 143 145\n",
    "    10 31 44 70 83 86 92 100 102 305 309 315 320 326 327 328\n",
    "    11 240 241 243 244 245 248 250 254 255 256 258 260 275 279 295\n",
    "    12 535 542 571 573 574 575 576 578 581 582 583 584 585 586 588\n",
    "    13 485 520 523 526 527 529 530 531 532 536 537 538 539 540 541\n",
    "    \"\"\"\n",
    "    folder_path = \"/Users/sophiehuang/Documents/113-1/113-1-IRTM/HW3/IRTM\"  # Replace with your folder path\n",
    "    output_path = \"output_tfidf_likelihood.csv\"  # The output file path\n",
    "\n",
    "    docs = load_documents(folder_path)\n",
    "    classify_and_output_with_tfidf_likelihood(\n",
    "        training_data, \n",
    "        docs, \n",
    "        output_path, \n",
    "        tfidf_features=1000,  # Number of features to keep after TF-IDF\n",
    "        lr_features=500       # Number of features to keep after Likelihood Ratio\n",
    "    )\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
